{
 "metadata": {
  "name": "",
  "signature": "sha256:547f945f39423435b661a796eac3b55ac63c2ca0e96551c61da00a0da1c2a308"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      ".. _reproduce_shalek2013:\n",
      "\n",
      ".. currentmodule:: flotilla"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Shalek and Satija, *et al* (2013)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In the interest of reproducibility, and to showcase our new package [`flotilla`](http://github.com/yeolab/flotilla), I've reproduced many figures from the landmark single-cell paper, [Single-cell transcriptomics reveals bimodality in expression and splicing in immune cells](http://www.ncbi.nlm.nih.gov/pubmed/23685454) by Shalek and Satija, *et al*. *Nature* (2013). In this paper, Regev and colleagues performed single-cell sequencing 18 bone marrow-derived dendritic cells (BMDCs), in addition to 3 pooled samples.\n",
      "\n",
      ".. note::\n",
      "    \n",
      "    This tutorial covers the dataset exploration, **not** the downloading, cleaning, and importing. For those steps, please refer to the `\"**Loading data from Shalek and Satija (2013)**\" <getting_started_shalek2013>`_ tutorial.\n",
      "\n",
      "Before we begin, let's import everything we need."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Turn on inline plots with IPython\n",
      "%matplotlib inline\n",
      "\n",
      "# Import the flotilla package for biological data analysis\n",
      "import flotilla\n",
      "\n",
      "# Import \"numerical python\" library for number crunching\n",
      "import numpy as np\n",
      "\n",
      "# Import \"panel data analysis\" library for tabular data\n",
      "import pandas as pd\n",
      "\n",
      "# Import statistical data visualization package\n",
      "# Note: As of November 6th, 2014, you will need the \"master\" version of \n",
      "# seaborn on github (v0.5.dev), installed via \n",
      "# \"pip install git+ssh://git@github.com/mwaskom/seaborn.git\n",
      "import seaborn as sns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Create a `flotilla` Study!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study = flotilla.embark(flotilla._shalek2013)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "To expose what's happening here, the ``embark`` command is loading data from a ``datapackage.json`` file as described at this location:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flotilla._shalek2013"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "'https://raw.githubusercontent.com/YeoLab/shalek2013/master/datapackage.json'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The `datapackage.json` file is what holds all the information relative to the study, and loosely follows the [datapackage spec](http://data.okfn.org/doc/data-package) created by the Open Knowledge Foundation.\n",
      "\n",
      "The data it's getting downloaded into ``$HOME/flotilla_projects/<study_name>/`` (the ``$HOME`` means my \"home directory\", which on my laptop is ``/Users/olga``). This will be saved in your home directory, too. We can take a look at the downloaded ``datapackage.json`` file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cat $HOME/flotilla_projects/shalek2013/datapackage.json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\r\n",
        "  \"name\": \"shalek2013\", \r\n",
        "  \"title\": null, \r\n",
        "  \"datapackage_version\": \"0.1.2\", \r\n",
        "  \"sources\": null, \r\n",
        "  \"licenses\": null, \r\n",
        "  \"resources\": [\r\n",
        "    {\r\n",
        "      \"path\": \"splicing.csv.gz\", \r\n",
        "      \"format\": \"csv\", \r\n",
        "      \"name\": \"splicing\", \r\n",
        "      \"compression\": \"gzip\"\r\n",
        "    }, \r\n",
        "    {\r\n",
        "      \"name\": \"mapping_stats\", \r\n",
        "      \"format\": \"csv\", \r\n",
        "      \"min_reads\": 500000.0, \r\n",
        "      \"path\": \"mapping_stats.csv.gz\", \r\n",
        "      \"number_mapped_col\": \"PF_READS\", \r\n",
        "      \"compression\": \"gzip\"\r\n",
        "    }, \r\n",
        "    {\r\n",
        "      \"name\": \"expression_feature\", \r\n",
        "      \"format\": \"csv\", \r\n",
        "      \"rename_col\": null, \r\n",
        "      \"ignore_subset_cols\": [], \r\n",
        "      \"path\": \"expression_feature.csv.gz\", \r\n",
        "      \"compression\": \"gzip\"\r\n",
        "    }, \r\n",
        "    {\r\n",
        "      \"name\": \"expression\", \r\n",
        "      \"log_base\": null, \r\n",
        "      \"format\": \"csv\", \r\n",
        "      \"thresh\": -Infinity, \r\n",
        "      \"plus_one\": false, \r\n",
        "      \"path\": \"expression.csv.gz\", \r\n",
        "      \"compression\": \"gzip\"\r\n",
        "    }, \r\n",
        "    {\r\n",
        "      \"name\": \"splicing_feature\", \r\n",
        "      \"format\": \"csv\", \r\n",
        "      \"rename_col\": \"gene_name\", \r\n",
        "      \"ignore_subset_cols\": [], \r\n",
        "      \"path\": \"splicing_feature.csv.gz\", \r\n",
        "      \"expression_id_col\": \"gene_name\", \r\n",
        "      \"compression\": \"gzip\"\r\n",
        "    }, \r\n",
        "    {\r\n",
        "      \"pooled_col\": \"pooled\", \r\n",
        "      \"name\": \"metadata\", \r\n",
        "      \"phenotype_to_marker\": {\r\n",
        "        \"immature\": \"o\", \r\n",
        "        \"mature\": \"^\"\r\n",
        "      }, \r\n",
        "      \"format\": \"csv\", \r\n",
        "      \"minimum_samples\": 0, \r\n",
        "      \"phenotype_to_color\": {\r\n",
        "        \"immature\": \"#f77189\", \r\n",
        "        \"mature\": \"#36ada4\", \r\n",
        "        \"BDMC\": \"#f77189\"\r\n",
        "      }, \r\n",
        "      \"outlier_col\": \"outlier\", \r\n",
        "      \"path\": \"metadata.csv.gz\", \r\n",
        "      \"phenotype_col\": \"maturity\", \r\n",
        "      \"phenotype_order\": [\r\n",
        "        \"immature\", \r\n",
        "        \"mature\"\r\n",
        "      ], \r\n",
        "      \"compression\": \"gzip\"\r\n",
        "    }, \r\n",
        "    {\r\n",
        "      \"name\": \"supplemental\", \r\n",
        "      \"resources\": []\r\n",
        "    }\r\n",
        "  ]\r\n",
        "}"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Let's look at what else is in this folder:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls /Users/olga/flotilla_projects/shalek2013"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "datapackage.json           expression_feature.csv     mapping_stats.csv.gz       splicing.csv               splicing_feature.csv.gz\r\n",
        "expression.csv             expression_feature.csv.gz  metadata.csv               splicing.csv.gz\r\n",
        "expression.csv.gz          mapping_stats.csv          metadata.csv.gz            splicing_feature.csv\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "So this is where all the other files are. Good to know!\n",
      "\n",
      "Now we can start creating figures!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Figure 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Here, we will attempt to re-create the sub-panels in [Figure 1](http://www.nature.com/nature/journal/v498/n7453/fig_tab/nature12172_F1.html), where the original is:\n",
      "\n",
      "![Original Figure 1](http://www.nature.com/nature/journal/v498/n7453/images/nature12172-f1.2.jpg)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 1a"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_two_samples('P1', 'P2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Without flotilla, you would do"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import seaborn as sns\n",
      "sns.set_style('ticks')\n",
      "\n",
      "x = expression_filtered.ix['P1']\n",
      "y = expression_filtered.ix['P2']\n",
      "jointgrid = sns.jointplot(x, y, joint_kws=dict(alpha=0.5))\n",
      "xmin, xmax, ymin, ymax = jointgrid.ax_joint.axis()\n",
      "jointgrid.ax_joint.set_xlim(0, xmax)\n",
      "jointgrid.ax_joint.set_ylim(0, ymax);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 1b"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Paper: $r=0.54$. Not sure at all what's going on here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_two_samples('S1', 'S2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Without `flotilla`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import seaborn as sns\n",
      "sns.set_style('ticks')\n",
      "\n",
      "x = expression_filtered.ix['S1']\n",
      "y = expression_filtered.ix['S2']\n",
      "jointgrid = sns.jointplot(x, y, joint_kws=dict(alpha=0.5))\n",
      "\n",
      "# Adjust xmin, ymin to 0\n",
      "xmin, xmax, ymin, ymax = jointgrid.ax_joint.axis()\n",
      "jointgrid.ax_joint.set_xlim(0, xmax)\n",
      "jointgrid.ax_joint.set_ylim(0, ymax);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By the way, you can do other kinds of plots with `flotilla`, like a kernel density estimate (\"`kde`\") plot:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_two_samples('S1', 'S2', kind='kde')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or a binned hexagon plot (\"`hexbin\"`):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_two_samples('S1', 'S2', kind='hexbin')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Any inputs that are valid to `seaborn`'s [`jointplot`](http://web.stanford.edu/~mwaskom/software/seaborn/generated/seaborn.jointplot.html#seaborn.jointplot) are valid."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 1c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = study.expression.data.ix['P1']\n",
      "y = study.expression.singles.mean()\n",
      "y.name = \"Average singles\"\n",
      "\n",
      "jointgrid = sns.jointplot(x, y, joint_kws=dict(alpha=0.5))\n",
      "\n",
      "# Adjust xmin, ymin to 0\n",
      "xmin, xmax, ymin, ymax = jointgrid.ax_joint.axis()\n",
      "jointgrid.ax_joint.set_xlim(0, xmax)\n",
      "jointgrid.ax_joint.set_ylim(0, ymax);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Figure 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Next, we will attempt to recreate the figures from [Figure 2](http://www.nature.com/nature/journal/v498/n7453/fig_tab/nature12172_F2.html):\n",
      "\n",
      "![Original figure 2](http://www.nature.com/nature/journal/v498/n7453/images/nature12172-f2.2.jpg)\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 2a\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "For this figure, we will need the \"LPS Response\" and \"Housekeeping\" gene annotations, from the `expression_feature_data` that we created."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get colors for plotting the gene categories\n",
      "dark2 = sns.color_palette('Dark2')\n",
      "\n",
      "singles = study.expression.singles\n",
      "# Get only gene categories for genes in the singles data\n",
      "singles, gene_categories = singles.align(study.expression.feature_data.gene_category, join='left', axis=1)\n",
      "\n",
      "mean = singles.mean()\n",
      "std = singles.std()\n",
      "\n",
      "jointgrid = sns.jointplot(mean, std, color='#262626', joint_kws=dict(alpha=0.5))\n",
      "\n",
      "for i, (category, s) in enumerate(gene_categories.groupby(gene_categories)):\n",
      "    jointgrid.ax_joint.plot(mean[s.index], std[s.index], 'o', color=dark2[i], markersize=5)\n",
      "\n",
      "jointgrid.ax_joint.set_xlabel('Standard deviation in single cells $\\mu$')\n",
      "jointgrid.ax_joint.set_ylabel('Average expression in single cells $\\sigma$')\n",
      "\n",
      "xmin, xmax, ymin, ymax = jointgrid.ax_joint.axis()\n",
      "vmax = max(xmax, ymax)\n",
      "vmin = min(xmin, ymin)\n",
      "jointgrid.ax_joint.plot([0, vmax], [0, vmax], color='steelblue')\n",
      "jointgrid.ax_joint.plot([0, vmax], [0, .25*vmax], color='grey')\n",
      "jointgrid.ax_joint.set_xlim(0, xmax)\n",
      "jointgrid.ax_joint.set_ylim(0, ymax)\n",
      "\n",
      "jointgrid.ax_joint.fill_betweenx((ymin, ymax), 0, np.log(250), alpha=0.5, zorder=-1);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I couldn't find the data for the `hESC`s for the right-side panel of Fig. 2a, so I couldn't remake the figure."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 2b"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "In the paper, they use *\"522 most highly expressed genes (single-cell average TPM > 250)\"*, but I wasn't able to replicate their numbers. If I use the pre-filtered expression data that I fed into flotilla, then I get 297 genes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean = study.expression.singles.mean()\n",
      "highly_expressed_genes = mean.index[mean > np.log(250 + 1)]\n",
      "len(highly_expressed_genes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which is much less. If I use the original, unfiltered data, then I get the *\"522\"* number, but this seems strange because they did the filtering step of *\"discarded genes not appreciably expressed (transcripts per million (TPM) > 1) in at least three individual cells, retaining 6,313 genes for further analysis\"*, and yet they went back to the original data to get this new subset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "expression.ix[:, expression.ix[singles_ids].mean() > 250].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "expression_highly_expressed = np.log(expression.ix[singles_ids, expression.ix[singles_ids].mean() > 250] + 1)\n",
      "\n",
      "mean = expression_highly_expressed.mean()\n",
      "\n",
      "std = expression_highly_expressed.std()\n",
      "\n",
      "mean_bins = pd.cut(mean, bins=np.arange(0, 11, 1))\n",
      "\n",
      "# Coefficient of variation\n",
      "cv = std/mean\n",
      "cv.sort()\n",
      "\n",
      "genes = mean.index\n",
      "\n",
      "\n",
      "# for name, df in shalek2013.expression.singles.groupby(dict(zip(genes, mean_bins)), axis=1):\n",
      "def calculate_cells_per_tpm_per_cv(df, cv):\n",
      "    df = df[df > 1]\n",
      "    df_aligned, cv_aligned = df.align(cv, join='inner', axis=1)\n",
      "    cv_aligned.sort()\n",
      "    n_cells = pd.Series(0, index=cv.index)\n",
      "    n_cells[cv_aligned.index] = df_aligned.ix[:, cv_aligned.index].count()\n",
      "    return n_cells\n",
      "\n",
      "grouped = expression_highly_expressed.groupby(dict(zip(genes, mean_bins)), axis=1)\n",
      "cells_per_tpm_per_cv = grouped.apply(calculate_cells_per_tpm_per_cv, cv=cv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's how you would make the original figure from the paper:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(10, 10))\n",
      "sns.heatmap(cells_per_tpm_per_cv, linewidth=0, ax=ax, yticklabels=False)\n",
      "ax.set_yticks([])\n",
      "ax.set_xlabel('ln(TPM, binned)');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Doesn't quite look the same. Maybe the y-axis labels were opposite, and higher up on the y-axis was less variant? Because I see a blob of color for (1,2] TPM (by the way, the figure in the paper is not TPM+1 as previous figures were)\n",
      "\n",
      "This is how you would make a modified version of the figure, which also plots the coefficient of variation on a side-plot, which I like because it shows the CV changes directly on the heatmap. Also, technically this is $\\ln$(TPM+1)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import gridspec\n",
      "\n",
      "fig = plt.figure(figsize=(12, 10))\n",
      "\n",
      "gs = gridspec.GridSpec(1, 2, wspace=0.01, hspace=0.01, width_ratios=[.2, 1])\n",
      "cv_ax = fig.add_subplot(gs[0, 0])\n",
      "heatmap_ax = fig.add_subplot(gs[0, 1])\n",
      "\n",
      "sns.heatmap(cells_per_tpm_per_cv, linewidth=0, ax=heatmap_ax)\n",
      "heatmap_ax.set_yticks([])\n",
      "heatmap_ax.set_xlabel('$\\ln$(TPM+1), binned')\n",
      "\n",
      "y = np.arange(cv.shape[0])\n",
      "cv_ax.set_xscale('log')\n",
      "cv_ax.plot(cv, y, color='#262626')\n",
      "cv_ax.fill_betweenx(cv, np.zeros(cv.shape), y, color='#262626', alpha=0.5)\n",
      "cv_ax.set_ylim(0, y.max())\n",
      "cv_ax.set_xlabel('CV = $\\mu/\\sigma$')\n",
      "cv_ax.set_yticks([])\n",
      "sns.despine(ax=cv_ax, left=True, right=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Figure 3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "We will attempt to re-create the sub-panel figures from [Figure 3](http://www.nature.com/nature/journal/v498/n7453/fig_tab/nature12172_F3.html):\n",
      "\n",
      "![Original Figure 3](http://www.nature.com/nature/journal/v498/n7453/images/nature12172-f3.2.jpg)\n",
      "\n",
      "Since we can't re-do the microscopy (Figure 3a) or the RNA-FISH counts (Figure 3c), we will make Figures 3b. These histograms are simple to do outside of `flotilla`, so we do not have them within flotilla."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 3b, top panel"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "sns.distplot(study.splicing.singles.values.flat, bins=np.arange(0, 1.05, 0.05), ax=ax)\n",
      "ax.set_xlim(0, 1)\n",
      "sns.despine()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 3b, bottom panel"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "sns.distplot(study.splicing.pooled.values.flat, bins=np.arange(0, 1.05, 0.05), ax=ax, color='grey')\n",
      "ax.set_xlim(0, 1)\n",
      "sns.despine()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Figure 4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "We will attempt to re-create the sub-panel figures from [Figure 4](http://www.nature.com/nature/journal/v498/n7453/fig_tab/nature12172_F4.html):\n",
      "\n",
      "![Original Figure 4](http://www.nature.com/nature/journal/v498/n7453/images/nature12172-f4.2.jpg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 4a"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Here, we can use the \"`interactive_pca`\" function we have to explore different dimensionality reductions on the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.interactive_pca()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A \"sequences shortened\" version of this is available as a gif:\n",
      "\n",
      "![Imgur](http://i.imgur.com/fJKPQ7W.gif)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Equivalently, I could have written out the plotting command by hand, instead of using `study.interactive_pca`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_pca(feature_subset='gene_category: LPS Response', sample_subset='not (pooled)', plot_violins=False, show_point_labels=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Mark immature cells as a new subset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As in the paper, the cells S12, S13, and S16 appear in a cluster that is separate from the remaining cells. From the paper, these were the \"matured\" bone-marrow derived dendritic cells, after stimulation with a lipopolysaccharide. We can mark these as mature in our metadata,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mature = ['S12', 'S13', 'S16']\n",
      "study.metadata.data['maturity'] = metadata.index.map(lambda x: 'mature' if x in mature else 'immature')\n",
      "study.metadata.data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, we can set **maturity** as the column we use for coloring the PCA, since before it was the \"phenotype\" column."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.metadata.phenotype_col = 'maturity'\n",
      "study.save('shalek2013')\n",
      "study = flotilla.embark('shalek2013')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_pca(feature_subset='gene_category: LPS Response', sample_subset='not (pooled)', plot_violins=False, show_point_labels=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.save('shalek2013')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Without `flotilla`, `plot_pca` is quite a bit of code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "from collections import defaultdict\n",
      "from itertools import cycle\n",
      "import math\n",
      "\n",
      "from sklearn import decomposition\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "import pandas as pd\n",
      "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "from flotilla.visualize.color import dark2\n",
      "from flotilla.visualize.generic import violinplot\n",
      "\n",
      "\n",
      "class DataFrameReducerBase(object):\n",
      "    \"\"\"\n",
      "\n",
      "    Just like scikit-learn's reducers, but with prettied up DataFrames.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, df, n_components=None, **decomposer_kwargs):\n",
      "\n",
      "        # This magically initializes the reducer like DataFramePCA or DataFrameNMF\n",
      "        if df.shape[1] <= 3:\n",
      "            raise ValueError(\n",
      "                \"Too few features (n={}) to reduce\".format(df.shape[1]))\n",
      "        super(DataFrameReducerBase, self).__init__(n_components=n_components,\n",
      "                                                   **decomposer_kwargs)\n",
      "        self.reduced_space = self.fit_transform(df)\n",
      "\n",
      "    def relabel_pcs(self, x):\n",
      "        return \"pc_\" + str(int(x) + 1)\n",
      "\n",
      "    def fit(self, X):\n",
      "        try:\n",
      "            assert type(X) == pd.DataFrame\n",
      "        except AssertionError:\n",
      "            sys.stdout.write(\"Try again as a pandas DataFrame\")\n",
      "            raise ValueError('Input X was not a pandas DataFrame, '\n",
      "                             'was of type {} instead'.format(str(type(X))))\n",
      "\n",
      "        self.X = X\n",
      "        super(DataFrameReducerBase, self).fit(X)\n",
      "        self.components_ = pd.DataFrame(self.components_,\n",
      "                                        columns=self.X.columns).rename_axis(\n",
      "            self.relabel_pcs, 0)\n",
      "        try:\n",
      "            self.explained_variance_ = pd.Series(\n",
      "                self.explained_variance_).rename_axis(self.relabel_pcs, 0)\n",
      "            self.explained_variance_ratio_ = pd.Series(\n",
      "                self.explained_variance_ratio_).rename_axis(self.relabel_pcs,\n",
      "                                                            0)\n",
      "        except AttributeError:\n",
      "            pass\n",
      "\n",
      "        return self\n",
      "\n",
      "    def transform(self, X):\n",
      "        component_space = super(DataFrameReducerBase, self).transform(X)\n",
      "        if type(self.X) == pd.DataFrame:\n",
      "            component_space = pd.DataFrame(component_space,\n",
      "                                           index=X.index).rename_axis(\n",
      "                self.relabel_pcs, 1)\n",
      "        return component_space\n",
      "\n",
      "    def fit_transform(self, X):\n",
      "        try:\n",
      "            assert type(X) == pd.DataFrame\n",
      "        except:\n",
      "            sys.stdout.write(\"Try again as a pandas DataFrame\")\n",
      "            raise ValueError('Input X was not a pandas DataFrame, '\n",
      "                             'was of type {} instead'.format(str(type(X))))\n",
      "        self.fit(X)\n",
      "        return self.transform(X)\n",
      "\n",
      "\n",
      "class DataFramePCA(DataFrameReducerBase, decomposition.PCA):\n",
      "    pass\n",
      "\n",
      "\n",
      "class DataFrameNMF(DataFrameReducerBase, decomposition.NMF):\n",
      "    def fit(self, X):\n",
      "        \"\"\"\n",
      "        duplicated fit code for DataFrameNMF because sklearn's NMF cheats for\n",
      "        efficiency and calls fit_transform. MRO resolves the closest\n",
      "        (in this package)\n",
      "        _single_fit_transform first and so there's a recursion error:\n",
      "\n",
      "            def fit(self, X, y=None, **params):\n",
      "                self._single_fit_transform(X, **params)\n",
      "                return self\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "            assert type(X) == pd.DataFrame\n",
      "        except:\n",
      "            sys.stdout.write(\"Try again as a pandas DataFrame\")\n",
      "            raise ValueError('Input X was not a pandas DataFrame, '\n",
      "                             'was of type {} instead'.format(str(type(X))))\n",
      "\n",
      "        self.X = X\n",
      "        # notice this is fit_transform, not fit\n",
      "        super(decomposition.NMF, self).fit_transform(X)\n",
      "        self.components_ = pd.DataFrame(self.components_,\n",
      "                                        columns=self.X.columns).rename_axis(\n",
      "            self.relabel_pcs, 0)\n",
      "        return self\n",
      "\n",
      "\n",
      "class DataFrameICA(DataFrameReducerBase, decomposition.FastICA):\n",
      "    pass\n",
      "\n",
      "class DecompositionViz(object):\n",
      "    \"\"\"\n",
      "    Plots the reduced space from a decomposed dataset. Does not perform any\n",
      "    reductions of its own\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, reduced_space, components_,\n",
      "                 explained_variance_ratio_,\n",
      "                 feature_renamer=None, groupby=None,\n",
      "                 singles=None, pooled=None, outliers=None,\n",
      "                 featurewise=False,\n",
      "                 order=None, violinplot_kws=None,\n",
      "                 data_type='expression', label_to_color=None,\n",
      "                 label_to_marker=None,\n",
      "                 scale_by_variance=True, x_pc='pc_1',\n",
      "                 y_pc='pc_2', n_vectors=20, distance='L1',\n",
      "                 n_top_pc_features=50, max_char_width=30):\n",
      "        \"\"\"Plot the results of a decomposition visualization\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        reduced_space : pandas.DataFrame\n",
      "            A (n_samples, n_dimensions) DataFrame of the post-dimensionality\n",
      "            reduction data\n",
      "        components_ : pandas.DataFrame\n",
      "            A (n_features, n_dimensions) DataFrame of how much each feature\n",
      "            contributes to the components (trailing underscore to be\n",
      "            consistent with scikit-learn)\n",
      "        explained_variance_ratio_ : pandas.Series\n",
      "            A (n_dimensions,) Series of how much variance each component\n",
      "            explains. (trailing underscore to be consistent with scikit-learn)\n",
      "        feature_renamer : function, optional\n",
      "            A function which takes the name of the feature and renames it,\n",
      "            e.g. from an ENSEMBL ID to a HUGO known gene symbol. If not\n",
      "            provided, the original name is used.\n",
      "        groupby : mapping function | dict, optional\n",
      "            A mapping of the samples to a label, e.g. sample IDs to\n",
      "            phenotype, for the violinplots. If None, all samples are treated\n",
      "            the same and are colored the same.\n",
      "        singles : pandas.DataFrame, optional\n",
      "            For violinplots only. If provided and 'plot_violins' is True,\n",
      "            will plot the raw (not reduced) measurement values as violin plots.\n",
      "        pooled : pandas.DataFrame, optional\n",
      "            For violinplots only. If provided, pooled samples are plotted as\n",
      "            black dots within their label.\n",
      "        outliers : pandas.DataFrame, optional\n",
      "            For violinplots only. If provided, outlier samples are plotted as\n",
      "            a grey shadow within their label.\n",
      "        featurewise : bool, optional\n",
      "            If True, then the \"samples\" are features, e.g. genes instead of\n",
      "            samples, and the \"features\" are the samples, e.g. the cells\n",
      "            instead of the gene ids. Essentially, the transpose of the\n",
      "            original matrix. If True, then violins aren't plotted. (default\n",
      "            False)\n",
      "        order : list-like\n",
      "            The order of the labels for the violinplots, e.g. if the data is\n",
      "            from a differentiation timecourse, then this would be the labels\n",
      "            of the phenotypes, in the differentiation order.\n",
      "        violinplot_kws : dict\n",
      "            Any additional parameters to violinplot\n",
      "        data_type : 'expression' | 'splicing', optional\n",
      "            For violinplots only. The kind of data that was originally used\n",
      "            for the reduction. (default 'expression')\n",
      "        label_to_color : dict, optional\n",
      "            A mapping of the label, e.g. the phenotype, to the desired\n",
      "            plotting color (default None, auto-assigned with the groupby)\n",
      "        label_to_marker : dict, optional\n",
      "            A mapping of the label, e.g. the phenotype, to the desired\n",
      "            plotting symbol (default None, auto-assigned with the groupby)\n",
      "        scale_by_variance : bool, optional\n",
      "            If True, scale the x- and y-axes by their explained_variance_ratio_\n",
      "            (default True)\n",
      "        {x,y}_pc : str, optional\n",
      "            Principal component to plot on the x- and y-axis. (default \"pc_1\"\n",
      "            and \"pc_2\")\n",
      "        n_vectors : int, optional\n",
      "            Number of vectors to plot of the principal components. (default 20)\n",
      "        distance : 'L1' | 'L2', optional\n",
      "            The distance metric to use to plot the vector lengths. L1 is\n",
      "            \"Cityblock\", i.e. the sum of the x and y coordinates, and L2 is\n",
      "            the traditional Euclidean distance. (default \"L1\")\n",
      "        n_top_pc_features : int, optional\n",
      "            THe number of top features from the principal components to plot.\n",
      "            (default 50)\n",
      "        max_char_width : int, optional\n",
      "            Maximum character width of a feature name. Useful for crazy long\n",
      "            feature IDs like MISO IDs\n",
      "        \"\"\"\n",
      "        self.reduced_space = reduced_space\n",
      "        self.components_ = components_\n",
      "        self.explained_variance_ratio_ = explained_variance_ratio_\n",
      "\n",
      "        self.singles = singles\n",
      "        self.pooled = pooled\n",
      "        self.outliers = outliers\n",
      "\n",
      "        self.groupby = groupby\n",
      "        self.order = order\n",
      "        self.violinplot_kws = violinplot_kws if violinplot_kws is not None \\\n",
      "            else {}\n",
      "        self.data_type = data_type\n",
      "        self.label_to_color = label_to_color\n",
      "        self.label_to_marker = label_to_marker\n",
      "        self.n_vectors = n_vectors\n",
      "        self.x_pc = x_pc\n",
      "        self.y_pc = y_pc\n",
      "        self.pcs = (self.x_pc, self.y_pc)\n",
      "        self.distance = distance\n",
      "        self.n_top_pc_features = n_top_pc_features\n",
      "        self.featurewise = featurewise\n",
      "        self.feature_renamer = feature_renamer\n",
      "        self.max_char_width = max_char_width\n",
      "\n",
      "        if self.label_to_color is None:\n",
      "            colors = cycle(dark2)\n",
      "\n",
      "            def color_factory():\n",
      "                return colors.next()\n",
      "\n",
      "            self.label_to_color = defaultdict(color_factory)\n",
      "\n",
      "        if self.label_to_marker is None:\n",
      "            markers = cycle(['o', '^', 's', 'v', '*', 'D', 'h'])\n",
      "\n",
      "            def marker_factory():\n",
      "                return markers.next()\n",
      "\n",
      "            self.label_to_marker = defaultdict(marker_factory)\n",
      "\n",
      "        if self.groupby is None:\n",
      "            self.groupby = dict.fromkeys(self.reduced_space.index, 'all')\n",
      "        self.grouped = self.reduced_space.groupby(self.groupby, axis=0)\n",
      "        if order is not None:\n",
      "            self.color_ordered = [self.label_to_color[x] for x in self.order]\n",
      "        else:\n",
      "            self.color_ordered = [self.label_to_color[x] for x in\n",
      "                                  self.grouped.groups]\n",
      "\n",
      "        self.loadings = self.components_.ix[[self.x_pc, self.y_pc]]\n",
      "\n",
      "        # Get the explained variance\n",
      "        if explained_variance_ratio_ is not None:\n",
      "            self.vars = explained_variance_ratio_[[self.x_pc, self.y_pc]]\n",
      "        else:\n",
      "            self.vars = pd.Series([1., 1.], index=[self.x_pc, self.y_pc])\n",
      "\n",
      "        if scale_by_variance:\n",
      "            self.loadings = self.loadings.multiply(self.vars, axis=0)\n",
      "\n",
      "        # sort features by magnitude/contribution to transformation\n",
      "        reduced_space = self.reduced_space[[self.x_pc, self.y_pc]]\n",
      "        farthest_sample = reduced_space.apply(np.linalg.norm, axis=0).max()\n",
      "        whole_space = self.loadings.apply(np.linalg.norm).max()\n",
      "        scale = .25 * farthest_sample / whole_space\n",
      "        self.loadings *= scale\n",
      "\n",
      "        ord = 2 if self.distance == 'L2' else 1\n",
      "        self.magnitudes = self.loadings.apply(np.linalg.norm, ord=ord)\n",
      "        self.magnitudes.sort(ascending=False)\n",
      "\n",
      "        self.top_features = set([])\n",
      "        self.pc_loadings_labels = {}\n",
      "        self.pc_loadings = {}\n",
      "        for pc in self.pcs:\n",
      "            x = self.components_.ix[pc].copy()\n",
      "            x.sort(ascending=True)\n",
      "            half_features = int(self.n_top_pc_features / 2)\n",
      "            if len(x) > self.n_top_pc_features:\n",
      "                a = x[:half_features]\n",
      "                b = x[-half_features:]\n",
      "                labels = np.r_[a.index, b.index]\n",
      "                self.pc_loadings[pc] = np.r_[a, b]\n",
      "            else:\n",
      "                labels = x.index\n",
      "                self.pc_loadings[pc] = x\n",
      "\n",
      "            self.pc_loadings_labels[pc] = labels\n",
      "            self.top_features.update(labels)\n",
      "\n",
      "    def __call__(self, ax=None, title='', plot_violins=True,\n",
      "                 show_point_labels=False,\n",
      "                 show_vectors=True,\n",
      "                 show_vector_labels=True,\n",
      "                 markersize=10, legend=True):\n",
      "        gs_x = 14\n",
      "        gs_y = 12\n",
      "\n",
      "        if ax is None:\n",
      "            self.reduced_fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
      "            gs = GridSpec(gs_x, gs_y)\n",
      "\n",
      "        else:\n",
      "            gs = GridSpecFromSubplotSpec(gs_x, gs_y, ax.get_subplotspec())\n",
      "            self.reduced_fig = plt.gcf()\n",
      "\n",
      "        ax_components = plt.subplot(gs[:, :5])\n",
      "        ax_loading1 = plt.subplot(gs[:, 6:8])\n",
      "        ax_loading2 = plt.subplot(gs[:, 10:14])\n",
      "\n",
      "        self.plot_samples(show_point_labels=show_point_labels,\n",
      "                          title=title, show_vectors=show_vectors,\n",
      "                          show_vector_labels=show_vector_labels,\n",
      "                          markersize=markersize, legend=legend,\n",
      "                          ax=ax_components)\n",
      "        self.plot_loadings(pc=self.x_pc, ax=ax_loading1)\n",
      "        self.plot_loadings(pc=self.y_pc, ax=ax_loading2)\n",
      "        sns.despine()\n",
      "        self.reduced_fig.tight_layout()\n",
      "\n",
      "        if plot_violins and not self.featurewise and self.singles is not None:\n",
      "            self.plot_violins()\n",
      "        return self\n",
      "\n",
      "    def shorten(self, x):\n",
      "        if len(x) > self.max_char_width:\n",
      "            return '{}...'.format(x[:self.max_char_width])\n",
      "        else:\n",
      "            return x\n",
      "\n",
      "    def plot_samples(self, show_point_labels=True,\n",
      "                     title='DataFramePCA', show_vectors=True,\n",
      "                     show_vector_labels=True, markersize=10,\n",
      "                     three_d=False, legend=True, ax=None):\n",
      "\n",
      "        \"\"\"\n",
      "        Given a pandas dataframe, performs DataFramePCA and plots the results in a\n",
      "        convenient single function.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        groupby : groupby\n",
      "            How to group the samples by color/label\n",
      "        label_to_color : dict\n",
      "            Group labels to a matplotlib color E.g. if you've already chosen\n",
      "            specific colors to indicate a particular group. Otherwise will\n",
      "            auto-assign colors\n",
      "        label_to_marker : dict\n",
      "            Group labels to matplotlib marker\n",
      "        title : str\n",
      "            title of the plot\n",
      "        show_vectors : bool\n",
      "            Whether or not to draw the vectors indicating the supporting\n",
      "            principal components\n",
      "        show_vector_labels : bool\n",
      "            whether or not to draw the names of the vectors\n",
      "        show_point_labels : bool\n",
      "            Whether or not to label the scatter points\n",
      "        markersize : int\n",
      "            size of the scatter markers on the plot\n",
      "        text_group : list of str\n",
      "            Group names that you want labeled with text\n",
      "        three_d : bool\n",
      "            if you want hte plot in 3d (need to set up the axes beforehand)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        For each vector in data:\n",
      "        x, y, marker, distance\n",
      "        \"\"\"\n",
      "        if ax is None:\n",
      "            ax = plt.gca()\n",
      "\n",
      "        # Plot the samples\n",
      "        for name, df in self.grouped:\n",
      "            color = self.label_to_color[name]\n",
      "            marker = self.label_to_marker[name]\n",
      "            x = df[self.x_pc]\n",
      "            y = df[self.y_pc]\n",
      "            ax.plot(x, y, color=color, marker=marker, linestyle='None',\n",
      "                    label=name, markersize=markersize, alpha=0.75,\n",
      "                    markeredgewidth=.1)\n",
      "            try:\n",
      "                if not self.pooled.empty:\n",
      "                    pooled_ids = x.index.intersection(self.pooled.index)\n",
      "                    pooled_x, pooled_y = x[pooled_ids], y[pooled_ids]\n",
      "                    ax.plot(pooled_x, pooled_y, 'o', color=color, marker=marker,\n",
      "                            markeredgecolor='k', markeredgewidth=2,\n",
      "                            label='{} pooled'.format(name),\n",
      "                            markersize=markersize, alpha=0.75)\n",
      "            except AttributeError:\n",
      "                pass\n",
      "            try:\n",
      "                if not self.outliers.empty:\n",
      "                    outlier_ids = x.index.intersection(self.outliers.index)\n",
      "                    outlier_x, outlier_y = x[outlier_ids], y[outlier_ids]\n",
      "                    ax.plot(outlier_x, outlier_y, 'o', color=color,\n",
      "                            marker=marker,\n",
      "                            markeredgecolor='lightgrey', markeredgewidth=5,\n",
      "                            label='{} outlier'.format(name),\n",
      "                            markersize=markersize, alpha=0.75)\n",
      "            except AttributeError:\n",
      "                pass\n",
      "            if show_point_labels:\n",
      "                for args in zip(x, y, df.index):\n",
      "                    ax.text(*args)\n",
      "\n",
      "        # Plot vectors, if asked\n",
      "        if show_vectors:\n",
      "            for vector_label in self.magnitudes[:self.n_vectors].index:\n",
      "                x, y = self.loadings[vector_label]\n",
      "                ax.plot([0, x], [0, y], color='k', linewidth=1)\n",
      "                if show_vector_labels:\n",
      "                    x_offset = math.copysign(5, x)\n",
      "                    y_offset = math.copysign(5, y)\n",
      "                    horizontalalignment = 'left' if x > 0 else 'right'\n",
      "                    if self.feature_renamer is not None:\n",
      "                        renamed = self.feature_renamer(vector_label)\n",
      "                    else:\n",
      "                        renamed = vector_label\n",
      "                    ax.annotate(renamed, (x, y),\n",
      "                                textcoords='offset points',\n",
      "                                xytext=(x_offset, y_offset),\n",
      "                                horizontalalignment=horizontalalignment)\n",
      "\n",
      "        # Label x and y axes\n",
      "        ax.set_xlabel(\n",
      "            'Principal Component {} (Explains {:.2f}% Of Variance)'.format(\n",
      "                str(self.x_pc), 100 * self.vars[self.x_pc]))\n",
      "        ax.set_ylabel(\n",
      "            'Principal Component {} (Explains {:.2f}% Of Variance)'.format(\n",
      "                str(self.y_pc), 100 * self.vars[self.y_pc]))\n",
      "        ax.set_title(title)\n",
      "\n",
      "        if legend:\n",
      "            ax.legend()\n",
      "        sns.despine()\n",
      "\n",
      "    def plot_loadings(self, pc='pc_1', n_features=50, ax=None):\n",
      "        loadings = self.pc_loadings[pc]\n",
      "        labels = self.pc_loadings_labels[pc]\n",
      "\n",
      "        if ax is None:\n",
      "            ax = plt.gca()\n",
      "\n",
      "        ax.plot(loadings, np.arange(loadings.shape[0]), 'o')\n",
      "\n",
      "        ax.set_yticks(np.arange(max(loadings.shape[0], n_features)))\n",
      "        ax.set_title(\"Component \" + pc)\n",
      "\n",
      "        x_offset = max(loadings) * .05\n",
      "        ax.set_xlim(left=loadings.min() - x_offset,\n",
      "                    right=loadings.max() + x_offset)\n",
      "\n",
      "        if self.feature_renamer is not None:\n",
      "            labels = map(self.feature_renamer, labels)\n",
      "        else:\n",
      "            labels = labels\n",
      "\n",
      "        labels = map(self.shorten, labels)\n",
      "        # ax.set_yticklabels(map(shorten, labels))\n",
      "        ax.set_yticklabels(labels)\n",
      "        for lab in ax.get_xticklabels():\n",
      "            lab.set_rotation(90)\n",
      "        sns.despine(ax=ax)\n",
      "\n",
      "    def plot_explained_variance(self, title=\"PCA explained variance\"):\n",
      "        \"\"\"If the reducer is a form of PCA, then plot the explained variance\n",
      "        ratio by the components.\n",
      "        \"\"\"\n",
      "        # Plot the explained variance ratio\n",
      "        assert self.explained_variance_ratio_ is not None\n",
      "        import matplotlib.pyplot as plt\n",
      "        import seaborn as sns\n",
      "\n",
      "        fig, ax = plt.subplots()\n",
      "        ax.plot(self.explained_variance_ratio_, 'o-')\n",
      "\n",
      "        xticks = np.arange(len(self.explained_variance_ratio_))\n",
      "        ax.set_xticks(xticks)\n",
      "        ax.set_xticklabels(xticks + 1)\n",
      "        ax.set_xlabel('Principal component')\n",
      "        ax.set_ylabel('Fraction explained variance')\n",
      "        ax.set_title(title)\n",
      "        sns.despine()\n",
      "\n",
      "    def plot_violins(self):\n",
      "        \"\"\"Make violinplots of each feature\n",
      "\n",
      "        Must be called after plot_samples because it depends on the existence\n",
      "        of the \"self.magnitudes\" attribute.\n",
      "        \"\"\"\n",
      "        ncols = 4\n",
      "        nrows = 1\n",
      "        vector_labels = list(set(self.magnitudes[:self.n_vectors].index.union(\n",
      "            pd.Index(self.top_features))))\n",
      "        while ncols * nrows < len(vector_labels):\n",
      "            nrows += 1\n",
      "        self.violins_fig, axes = plt.subplots(nrows=nrows, ncols=ncols,\n",
      "                                              figsize=(4 * ncols, 4 * nrows))\n",
      "\n",
      "        if self.feature_renamer is not None:\n",
      "            renamed_vectors = map(self.feature_renamer, vector_labels)\n",
      "        else:\n",
      "            renamed_vectors = vector_labels\n",
      "        labels = [(y, x) for (y, x) in sorted(zip(renamed_vectors,\n",
      "                                                  vector_labels))]\n",
      "\n",
      "        for (renamed, feature_id), ax in zip(labels, axes.flat):\n",
      "            singles = self.singles[feature_id] if self.singles is not None \\\n",
      "                else None\n",
      "            pooled = self.pooled[feature_id] if self.pooled is not None else \\\n",
      "                None\n",
      "            outliers = self.outliers[feature_id] if self.outliers is not None \\\n",
      "                else None\n",
      "            title = '{}\\n{}'.format(feature_id, renamed)\n",
      "            violinplot(singles, pooled_data=pooled, outliers=outliers,\n",
      "                       groupby=self.groupby, color_ordered=self.color_ordered,\n",
      "                       order=self.order, title=title,\n",
      "                       ax=ax, data_type=self.data_type,\n",
      "                       **self.violinplot_kws)\n",
      "\n",
      "        # Clear any unused axes\n",
      "        for ax in axes.flat:\n",
      "            # Check if the plotting space is empty\n",
      "            if len(ax.collections) == 0 or len(ax.lines) == 0:\n",
      "                ax.axis('off')\n",
      "        self.violins_fig.tight_layout()\n",
      "\n",
      "# Notice we're using the original data, nothing from \"study\"\n",
      "lps_response_genes = expression_feature_data.index[expression_feature_data.gene_category == 'LPS Response']\n",
      "subset = expression_filtered.ix[singles_ids, lps_response_genes].dropna(how='all', axis=1)\n",
      "subset_standardized = pd.DataFrame(StandardScaler().fit_transform(subset),\n",
      "                                       index=subset.index, columns=subset.columns)\n",
      "\n",
      "\n",
      "pca = DataFramePCA(subset_standardized)\n",
      "visualizer = DecompositionViz(pca.reduced_space, pca.components_, pca.explained_variance_ratio_)\n",
      "visualizer();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 4b"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lps_response_genes = study.expression.feature_subsets['gene_category: LPS Response']\n",
      "lps_response = study.expression.singles.ix[:, lps_response_genes].dropna(how='all', axis=1)\n",
      "lps_response.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lps_response_corr = lps_response.corr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "\"Elbow method\" for determining number of clusters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The authors state that they used the \"Elbow method\" to determine the [number of cluster centers](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set). Essentially, you try a bunch of different $k$, and see where there is a flattening out of the metric, like an elbow. There's a few different variations on which metric to use, such as using the average distance to the cluster center, or the explained variance. Let's try the distance to cluster center first, because `scikit-learn` makes it easy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import KMeans\n",
      "\n",
      "##### cluster data into K=1..10 clusters #####\n",
      "ks = np.arange(1, 11).astype(int)\n",
      "\n",
      "X = lps_response_corr.values\n",
      "\n",
      "kmeans = [KMeans(n_clusters=k).fit(X) for k in ks]\n",
      "\n",
      "# Scikit-learn makes this easy by computing the distance to the nearest center\n",
      "dist_to_center = [km.inertia_ for km in kmeans]\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(ks, dist_to_center, 'o-')\n",
      "ax.set_ylabel('Sum of distance to nearest cluster center')\n",
      "sns.despine()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not quite sure where the elbow is here. looks like there's a big drop off after $k=1$, but that could just be an illusion. Since they didn't specify which version of the elbow method they used, I'm not going to investigate this further, and just see if we can see what they see with the $k=5$ clusters that they found was optimal.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans = KMeans(n_clusters=5)\n",
      "lps_response_corr_clusters = kmeans.fit_predict(lps_response_corr.values)\n",
      "lps_response_corr_clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's create a dataframe with these genes in their cluster orders."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_to_cluster = dict(zip(lps_response_corr.columns, lps_response_corr_clusters))\n",
      "\n",
      "dfs = []\n",
      "for name, df in lps_response_corr.groupby(gene_to_cluster):\n",
      "    dfs.append(df)\n",
      "lps_response_corr_ordered_by_clusters = pd.concat(dfs)\n",
      "\n",
      "# Make symmetric, since we created this dataframe by smashing rows on top of each other, we need to reorder the columns\n",
      "lps_response_corr_ordered_by_clusters = lps_response_corr_ordered_by_clusters.ix[:, lps_response_corr_ordered_by_clusters.index]\n",
      "lps_response_corr_ordered_by_clusters.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next step is to get the principal-component reduced data, using only the LPS response genes. We can do this in `flotilla` using `study.expression.reduce`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduced = study.expression.reduce(singles_ids, feature_ids=lps_response_genes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can get the principal components using `reduced.components_` (similar interface as `scikit-learn`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduced.components_.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pc_components = reduced.components_.ix[:2, lps_response_corr_ordered_by_clusters.index].T\n",
      "pc_components.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib as mpl\n",
      "\n",
      "fig = plt.figure(figsize=(12, 10))\n",
      "gs = gridspec.GridSpec(2, 2, wspace=0.1, hspace=0.1, width_ratios=[1, .2], height_ratios=[1, .1])\n",
      "corr_ax = fig.add_subplot(gs[0, 0])\n",
      "corr_cbar_ax = fig.add_subplot(gs[1, 0])\n",
      "pc_ax = fig.add_subplot(gs[0, 1:])\n",
      "pc_cbar_ax = fig.add_subplot(gs[1:, 1:])\n",
      "\n",
      "sns.heatmap(lps_response_corr_ordered_by_clusters, linewidth=0, ax=corr_ax, cbar_ax=corr_cbar_ax, \n",
      "            cbar_kws=dict(orientation='horizontal'))\n",
      "sns.heatmap(pc_components, cmap=mpl.cm.PRGn, linewidth=0, ax=pc_ax, cbar_ax=pc_cbar_ax,\n",
      "            cbar_kws=dict(orientation='horizontal'))\n",
      "\n",
      "corr_ax.set_xlabel('')\n",
      "corr_ax.set_ylabel('')\n",
      "corr_ax.set_xticks([])\n",
      "corr_ax.set_yticks([])\n",
      "pc_ax.set_yticks([])\n",
      "pc_ax.set_ylabel('')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This looks pretty similar, maybe just rearranged cluster order. Let's check what their data looks like when you plot this."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Their PC scores and clusters for the genes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_pc_clusters = pd.read_excel('nature12172-s1/Supplementary_Table5.xls', index_col=0)\n",
      "gene_pc_clusters.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = lps_response_corr.ix[gene_pc_clusters.index, gene_pc_clusters.index].dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
      "\n",
      "fig = plt.figure(figsize=(12, 10))\n",
      "gs = gridspec.GridSpec(2, 2, wspace=0.1, hspace=0.1, width_ratios=[1, .2], height_ratios=[1, .1])\n",
      "corr_ax = fig.add_subplot(gs[0, 0])\n",
      "corr_cbar_ax = fig.add_subplot(gs[1, 0])\n",
      "pc_ax = fig.add_subplot(gs[0, 1:])\n",
      "pc_cbar_ax = fig.add_subplot(gs[1:, 1:])\n",
      "\n",
      "sns.heatmap(data, linewidth=0, square=True, vmin=-1, vmax=1, ax=corr_ax, cbar_ax=corr_cbar_ax, cbar_kws=dict(orientation='horizontal'))\n",
      "sns.heatmap(gene_pc_clusters.ix[:, ['PC1 Score', 'PC2 Score']], linewidth=0, cmap=mpl.cm.PRGn,\n",
      "            ax=pc_ax, cbar_ax=pc_cbar_ax, cbar_kws=dict(orientation='horizontal'), xticklabels=False, yticklabels=False)\n",
      "\n",
      "corr_ax.set_xlabel('')\n",
      "corr_ax.set_ylabel('')\n",
      "corr_ax.set_xticks([])\n",
      "corr_ax.set_yticks([])\n",
      "\n",
      "pc_ax.set_yticks([])\n",
      "pc_ax.set_ylabel('');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sure enough, if I use their annotations, I get exactly that. Though there were two genes in their file that I didn't have in the `lps_response_corr` data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_pc_clusters.index.difference(lps_response_corr.index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Oh joy, another `datetime` error, just like we had with `expression2`... Looking back at the original Excel file, there is one gene that Excel mangled to be a date:\n",
      "\n",
      "![](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_supplementary_table_5_datetime_error.png)\n",
      "\n",
      "Please, can we start using just plain ole `.csv`s for supplementary data! Excel does NOT preserve strings if they start with numbers, and instead thinks they are dates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "collections.Counter(gene_pc_clusters.index.map(type))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Yep, it's just that one that got mangled.... oh well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_pc_clusters_genes = set(filter(lambda x: isinstance(x, unicode), gene_pc_clusters.index))\n",
      "gene_pc_clusters_genes.difference(lps_response_corr.index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, \"`RPS6KA2`\" is the only gene that was in their list of genes and not in mine."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Supplementary figures"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we get to have even more fun by plotting the Supplementary figures! :D\n",
      "\n",
      "Ironically, the supplementary figures are usually way easier to access (like not behind a paywall), and yet they're usually the documents that really have the crucial information about the experiments."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figure 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Supplementary figure 1, a correlation plot](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig1.png)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "singles_mean = study.expression.singles.mean()\n",
      "singles_mean.name = 'Single cell average'\n",
      "\n",
      "# Need to convert \"average_singles\" to a DataFrame instead of a single-row Series\n",
      "singles_mean = pd.DataFrame(singles_mean)\n",
      "singles_mean.head()\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_for_correlations = pd.concat([study.expression.singles, singles_mean.T, study.expression.pooled])\n",
      "\n",
      "# Take the transpose of the data, because the plotting algorithm calculates correlations between columns,\n",
      "# And we want the correlations between samples, not features\n",
      "data_for_correlations = data_for_correlations.T\n",
      "data_for_correlations.head()\n",
      "\n",
      "# %time sns.corrplot(data_for_correlations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(10, 10))\n",
      "sns.corrplot(data_for_correlations, ax=ax)\n",
      "sns.despine()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that this is mostly red, while in the figure from the paper, it was both blue and red. This is because the colormap started at 0.2 (not negative), and was centered with white at about 0.6. I see that they're trying to emphasize how much more correlated the pooled samples are to each other, but I think a simple sequential map would have been more effective."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figures 2 and 3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supplementary [Figure 2](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig2.png) and [Figure 3](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig3.png) are from FISH and raw sequence data, and are out of the scope of this computational reproduction."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figure 4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Supplementary Figure 4](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig4.png) was from published data, however the citation in the Supplementary Information (#23) was a [machine-learning book](http://link.springer.com/book/10.1007%2F978-3-642-51175-2), and #23 in the main text citations was a [review of probabilistic graphical models](http://www.sciencemag.org/content/303/5659/799.full), neither of which have the mouse embryonic stem cells or mouse embryonic fibroblasts used in the figure.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figure 5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this figure, we can only plot 5d, since it's derived directly from a table in their dataset.\n",
      "\n",
      "Warning: these data are going to require some serious cleaning. Yay data janitorial duties!\n",
      "\n",
      "![](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig5.png)\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Supplementary Figure 5d"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded = pd.read_excel('nature12172-s1/Supplementary_Table7.xlsx')\n",
      "barcoded.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first three columns are TPM calculated from the three samples that have molecular barcodes, and the last three columns are the integer counts of molecular barcodes from the three molecular barcode samples.\n",
      "\n",
      "Let's remove the \"Unnamed: 3\" column which is all NaNs. We'll do that with the `.dropna` method, specifying `axis=1` for columns and `how=\"all\"` to make sure only columns that have ALL NaNs are removed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded = barcoded.dropna(how='all', axis=1)\n",
      "barcoded.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let's drop that pesky \"GENE\" row. Don't worry, we'll get the sample ID names back next."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded = barcoded.drop('GENE', axis=0)\n",
      "barcoded.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll create a `pandas.MultiIndex` from the tuples of `(sample_id, measurement_type)` pair."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns = pd.MultiIndex.from_tuples([('MB_S1', 'TPM'),\n",
      "           ('MB_S2', 'TPM'),\n",
      "           ('MB_S3', 'TPM'),\n",
      "           ('MB_S1', 'Unique Barcodes'),\n",
      "           ('MB_S2', 'Unique Barcodes'),\n",
      "           ('MB_S3', 'Unique Barcodes')])\n",
      "barcoded.columns = columns\n",
      "barcoded = barcoded.sort_index(axis=1)\n",
      "barcoded.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the next move, we're going to do some crazy `pandas`-fu. First we're going to transpose, then `reset_index` of the transpose. Just so you know what this looks like, it's this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded.T.reset_index().head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we're going to transform the data into a [tidy](http://vita.had.co.nz/papers/tidy-data.pdf) format, with separate columns for sample ids, measurement types, the gene that was measured, and its measurement value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy = pd.melt(barcoded.T.reset_index(), id_vars=['level_0', 'level_1'])\n",
      "barcoded_tidy.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's rename these columns into something more useful, instead of \"level_0\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy = barcoded_tidy.rename(columns={'level_0': 'sample_id', 'level_1': 'measurement', 'variable': 'gene_name'})\n",
      "barcoded_tidy.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we're going to take some seemingly-duplicating steps, but trust me, it'll make the data easier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy['TPM'] = barcoded_tidy.value[barcoded_tidy.measurement == 'TPM']\n",
      "barcoded_tidy['Unique Barcodes'] = barcoded_tidy.value[barcoded_tidy.measurement == 'Unique Barcodes']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fill the values of the \"**TPM**\"'s forwards, since they appear first, and fill the values of the \"**Unique Barcodes**\" backwards, since they're second"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy.TPM = barcoded_tidy.TPM.ffill()\n",
      "barcoded_tidy['Unique Barcodes'] = barcoded_tidy['Unique Barcodes'].bfill()\n",
      "barcoded_tidy.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Drop the \"**measurement**\" column and drop duplicate rows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy = barcoded_tidy.drop('measurement', axis=1)\n",
      "barcoded_tidy = barcoded_tidy.drop_duplicates()\n",
      "barcoded_tidy.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy['log TPM'] = np.log(barcoded_tidy.TPM)\n",
      "barcoded_tidy['log Unique Barcodes'] = np.log(barcoded_tidy['Unique Barcodes'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use the convenient linear model plot (`lmplot`) in `seaborn` to plot these three samples together!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.lmplot('log TPM', 'log Unique Barcodes', barcoded_tidy, col='sample_id')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figures 6-20"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supplementary Figures [6](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig6.png), \n",
      "[7](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig7.png),\n",
      "[8](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig8.png), \n",
      "[9](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig9.png), \n",
      "[10](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig10.png), \n",
      "[11](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig11.png), \n",
      "[12](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig12.png), \n",
      "[13](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig13.png), \n",
      "[14](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig14.png), \n",
      "[15](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig15.png), \n",
      "[16](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig16.png), \n",
      "[17](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig17.png), \n",
      "[18](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig18.png), \n",
      "[19](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig19.png), and\n",
      "[20](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig20.png), \n",
      "deal with splicing data from the molecular barcodes, RNA-FISH, flow-sorted cells, and single-cell RT-PCR and are out of the scope of this reproduction."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusions\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While there may be minor, undocumented, differences between the methods presented in the manuscript and the figures, the application of [`flotilla`](https://github.com/YeoLab/flotilla) presents an opportunity to avoid these types of inconsistencies by strictly documenting every change to code and every transformation of the data. The biology the authors found is clearly real, as they did the knockout experiment of *Ifnr-/-* and saw that indeed the maturation process was affected, and *Stat2* and *Irf7* had much lower expression, as with the \"maturing\" cells in the data."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}